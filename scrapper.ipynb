{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67aee907-f3b5-46b8-bd0f-e2c12eb243e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "            return links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {url}: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while retrieving page {url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def normalize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    scheme = parsed_url.scheme.lower()\n",
    "    netloc = parsed_url.netloc.lower().replace('www.', '')  # Remove 'www.' if present\n",
    "    path = parsed_url.path.rstrip('/')  # Remove trailing slashes\n",
    "    normalized_url = f\"{scheme}://{netloc}{path}\"\n",
    "    return normalized_url\n",
    "\n",
    "def filter_links(links, base_domain):\n",
    "    valid_links = []\n",
    "    for link in links:\n",
    "        if link is None:\n",
    "            continue\n",
    "        parsed_url = urlparse(link)\n",
    "        normalized_url = normalize_url(link)\n",
    "        if base_domain in parsed_url.netloc.lower() or base_domain in parsed_url.path.lower():\n",
    "            if not any(normalized_url.lower().endswith(ext) for ext in ('.jpg', '.png', '.gif', '.mp4', '.avi', '.mp3')):\n",
    "                valid_links.append(normalized_url)\n",
    "    return valid_links\n",
    "\n",
    "def scrape_website(url, base_domain, depth, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if depth == 0 or url in visited:\n",
    "        return [url]\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    links = get_links(url)\n",
    "    filtered_links = filter_links(links, base_domain)\n",
    "\n",
    "    collected_links = [url]\n",
    "    for link in filtered_links:\n",
    "        absolute_url = urljoin(url, link)\n",
    "        collected_links.extend(scrape_website(absolute_url, base_domain, depth - 1, visited))\n",
    "\n",
    "    return list(set(collected_links))\n",
    "\n",
    "# Example usage:\n",
    "# url = \"https://camereye.ai/\"\n",
    "# base_domain = urlparse(url).netloc.split('.')[0]\n",
    "# links = scrape_website(url, base_domain, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fb455b-96f3-4b96-9401-5e2b9e1ede10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://camereye.ai/\"\n",
    "base_domain = urlparse(url).netloc.split('.')[0]\n",
    "links = scrape_website(url, base_domain, 2)\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfde7a3-027c-4d55-bf0b-64113964c72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dacfd7-9a1c-429e-a32c-fe1807cb753d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c1ee0-1666-4438-9502-e565e2017f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e3885-8092-49a9-a23a-28200ec47f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb42291-62c0-43c6-869b-07e67d381349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader, UnstructuredWordDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def get_vectorstore_from_file(files):\n",
    "    all_document_chunks = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        # Load document based on file extension\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "        if file_extension == 'pdf':\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_extension == 'txt':\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_extension == 'csv':\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_extension == 'doc' or file_extension == 'docx':\n",
    "            loader = UnstructuredWordDocumentLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_extension}\")\n",
    "            continue\n",
    "\n",
    "        # Load document\n",
    "        document = loader.load()\n",
    "\n",
    "        # Split document into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter()\n",
    "        document_chunks = text_splitter.split_documents(document)\n",
    "        all_document_chunks.extend(document_chunks)\n",
    "\n",
    "    return all_document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d24b742a-235e-4162-b5e7-0d32488aff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./files/1712046420938OWetnBnUMQX5QsqA.pdf', './files/Introducing Natoe.ai.pdf', './files/OutdoorClothingCatalog_1000.csv', './files/S-C-Invitation Letter.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7ef29b-c5b5-4f0e-95a4-573998bfb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = get_vectorstore_from_file(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a465a77-4fed-48ef-bec5-3844cf3405a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d245b-c353-4393-ad18-33b7cff5d1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cae8c0-214d-4a9a-986a-75c81616ecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97dce8b-e76a-4aab-a58d-4f10d5ab20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28f233-a761-4acf-b985-9764a27cd26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
